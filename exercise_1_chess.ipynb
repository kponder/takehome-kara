{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Chess\n",
    "\n",
    "The file `chess_games.csv` is a collection of chess games from Lichess and Chess.com along with a collection of metrics \n",
    "of each player's performance during the game (the actual game is not included).\n",
    "\n",
    "The task is this: __Determine the difference in player behavior when they’re winning vs losing vs maintaining their elo score__\n",
    "\n",
    "Be sure to look at the problem carefully, while we are looking at overall patterns amongst players, *how* you break up a \n",
    "player's performance into winning, losing, and maintaining is important. \n",
    "\n",
    "\n",
    "Note: What is the Elo score?\n",
    "- The elo score is a ranking based on if you won a match, the number of matches you played, and the elo score of your opponent.\n",
    "- Roughly = {score of person you played} + 400 * {win: 1, lose: -1, draw: 0} averaged over all games."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "* [Load Data and Run Exploratory Data Analysis](#eda)\n",
    "    * [Basic data/dataframe checks](#data)\n",
    "    * [Examine Categorical columns](#categorical)\n",
    "    * [Create EDA Plots](#plots)\n",
    "* [What does it mean to be Winning, Losing, or Maintaining?](#win_lose_main)\n",
    "    * [Feature engineering](#feat_eng)\n",
    "    * [Plot timeseries](#timeseries)\n",
    "    * [Slope Calculation](#slope)\n",
    "        * [Side bar: Elo score: Can I calculate it?](#calculate-elo)\n",
    "* [ Different behaviors of different groups](#behavior)\n",
    "    * [Histograms, medians, and means](#hist)\n",
    "    * [Common opening moves](#eco)\n",
    "    * [How do errors evolve within a game](#gameplay)\n",
    "    * [Difference between Time Classes](#timeclass)\n",
    "    * [Statistical Tests](#hypothesis)\n",
    "* [Summary of Results](#summary)\n",
    "* [Bonus: ML Classification](#bonus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import basic packages\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data and Run Exploratory Data Analysis (EDA) <a class=\"anchor\" id=\"eda\"></a>\n",
    "\n",
    "Here are some notes about the data to serve as a reference while I work through the dataset.\n",
    "- When Outcome \n",
    "    - 1: White won\n",
    "    - 0: black won\n",
    "    - 2: draw\n",
    "- Elo Score = `rating`\n",
    "- Service: location -> Note: prompt says there are two data sources, but data shows only data from Lichess\n",
    "- Eco Name: Name of opening move\n",
    "- Eco Code: Classification system for the opening moves\n",
    "- Eco Category: Classification system for the opening moves\n",
    "- ACL = average centipaw loss: the score of engine less the score of your move. lower score is better\n",
    "\n",
    "Action items:\n",
    "- Basic Data Checks\n",
    "    * Look at data and columns\n",
    "    * Check for Nulls\n",
    "- Examine categorical columns\n",
    "- Plot histogram of data and examine correlations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in Data\n",
    "chess = pd.read_csv('chess_games.csv',  sep='\\t')\n",
    "chess.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic data/dataframe checks <a class=\"anchor\" id=\"data\"></a>\n",
    "* Look at data and columns\n",
    "* Check for Nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run a basic check on the data frame to see the data types and nulls\n",
    "chess.info()\n",
    "# some of these are Null, so need to look at Nulls. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timestamp is an int column, but it can be more helpful to look at it in real date time.\n",
    "chess[\"datetime\"] = pd.to_datetime(chess[\"timestamp\"], unit='s')\n",
    "chess.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the number of unique chess players? There are 2000 possible unique players (one white, one black)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(list(chess.black_username) + list(chess.white_username)))\n",
    "# 847/2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If players play an equal number of games and there are only 847 players out of 2000 possible slots, do we use timestamps? Can we use time series to determine evolution of winning/losing/maintaining when this would indicate only 2-3 games?\n",
    "\n",
    "To answer this question, I counted how many entries per player for white and black. We see that `ieshuaganocry`, `hooligandi`, and `aimchess_bot` are the most prolific players. I will be able to use time as a way to determine \"winning\", \"losing\", \"maintain\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chess.groupby(\"black_username\").timestamp.count().reset_index().sort_values(by='timestamp', ascending=False).head(10), '\\n\\n')\n",
    "print(chess.groupby(\"white_username\").timestamp.count().reset_index().sort_values(by='timestamp', ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many players have more than one entry?\n",
    "white_username = chess.groupby(\"white_username\").timestamp.count().reset_index().rename(columns={'timestamp': \"count\", \"white_username\": \"username\"})\n",
    "black_username = chess.groupby(\"black_username\").timestamp.count().reset_index().rename(columns={'timestamp': \"count\", \"black_username\": \"username\"})\n",
    "\n",
    "usernames = pd.concat([white_username, black_username])\n",
    "usernames = usernames.groupby('username').sum().reset_index()\n",
    "print(\"# > 1:\", len(usernames[usernames[\"count\"] > 1]))\n",
    "print(\"# > 2:\", len(usernames[usernames[\"count\"] > 2]))\n",
    "print(\"# > 3:\", len(usernames[usernames[\"count\"] > 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only 80 users have more than 1 game, only 28 have more than 2, and only 13 have more than 3 games. This will limit the robustness of the split because there is limited data to run a time series analysis on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical columns <a class=\"anchor\" id=\"categorical\"></a>\n",
    "\n",
    "- What types of games are there?\n",
    "- What are the different services (expect 2)\n",
    "- What is ECO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# different types of chess play\n",
    "chess.groupby([\"time_class\"]).timestamp.count().reset_index().rename(columns={'timestamp': 'count'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The time class is dominated by `blitz` with `rapid` a distant second. Likely will not get good statistics on bullet or classic given the low number counts.\n",
    "\n",
    "Below I checked what websites were used. The prompt says there are two, but I only see one in the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where does the data come from? Looks like it is actually all from one service. \n",
    "chess[\"service\"].unique() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eco is a way to name and categories opening moves and games.\n",
    "Based on the analysis below, `eco_name` is the most granular and most descriptive while `eco_category` places the options into broader categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chess.groupby([\"eco_name\", \"eco_code\", \"eco_category\"]).timestamp.count().reset_index().sort_values(\"eco_code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in [\"eco_name\", \"eco_code\", \"eco_category\"]:\n",
    "    print(col, len(chess[col].unique()))\n",
    "\n",
    "# different levels of specificity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create EDA Plots <a class=\"anchor\" id=\"plots\"></a>\n",
    "\n",
    "- Histograms of the distributions of all columns\n",
    "- Correlation Matrix across all entries to understand what might be related.\n",
    "\n",
    "\n",
    "To start, I cannot look at histograms of categorical variables using pandas `hist` function. I will ordinally encode them (give them a number 0 - max(category)) and see broadly where the density is centered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of categorical columns to encode \n",
    "from sklearn.compose import make_column_selector as selector\n",
    "\n",
    "categorical_columns_selector = selector(dtype_include=object)\n",
    "categorical_columns = categorical_columns_selector(chess)\n",
    "categorical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "encoder = OrdinalEncoder().set_output(transform=\"pandas\")\n",
    "encoded_names = []\n",
    "\n",
    "# ignore username from the list\n",
    "for col in [\"time_class\", \"eco_name\", \"eco_code\", \"eco_category\"]:\n",
    "    _column = chess[[col]]\n",
    "    _encoded = encoder.fit_transform(_column)\n",
    "    chess[f'{col}_encoded'] = _encoded\n",
    "    encoded_names.append(f'{col}_encoded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas has an automatic way to check all histograms. I am blocking out the categorical columns that I already encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(25,25))\n",
    "ax = fig.gca()\n",
    "\n",
    "_ = chess.loc[:, ~chess.columns.isin(categorical_columns + ['timestamp'])].hist(ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Looking through the histograms, we can see that the data is well behaved without large quantities of outliers.\n",
    "- There are similar numbers of wins for white versus loses for white, but draws are less likely.\n",
    "- There are several metrics that are binary 0 or 1 entries (typically `winrate`s)\n",
    "- The `eco`s do appear to have peaks indicating that some opening moves are preferred.\n",
    "- White and black distributions should roughly be the same assuming white and black are assigned randomly.\n",
    "\n",
    "\n",
    "Now on to the correlation matrix. Again avoiding the categoricals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = chess.loc[:, ~chess.columns.isin(categorical_columns + ['timestamp'])].corr()\n",
    "\n",
    "fig = plt.figure(figsize=(15,15))\n",
    "ax = fig.gca()\n",
    "\n",
    "sns.heatmap(correlation_matrix, xticklabels=correlation_matrix.columns, yticklabels=correlation_matrix.columns, ax=ax)\n",
    "\n",
    "# Complex range of correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see complex correlations across most of the entries.\n",
    "\n",
    "Let's take a closer look at what is correlated with outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(2, 15))\n",
    "ax = fig.gca()\n",
    "\n",
    "sns.heatmap(correlation_matrix.iloc[:, 0:1], yticklabels=correlation_matrix.columns, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is interesting to note here that there is a strong negative correlation to the output with the black win rate, but much less strong correlations with the white win rate (which are opposite of black and positive.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What does it mean to be Winning, Losing, or Maintaining? <a class=\"anchor\" id=\"win_lose_main\"></a>\n",
    "\n",
    "\n",
    "Naively, could assign:\n",
    "- Winning match = conditions for winning\n",
    "- Losing match = conditions for losing\n",
    "- Draw = conditions for maintaining\n",
    "\n",
    "However, the prompt says examine `player behavior when they’re winning vs losing vs maintaining their elo score`.\n",
    "Does one win constitue \"winning\"? Given the wording in the prompt, I will assume that we need multiple wins to define \"winning\".\n",
    "\n",
    "\n",
    "Choices to define the groups:\n",
    "1. Multipe wins, loses, or draws (maintain) in a row. If so, how many?\n",
    "2. Postive, negative, same elo scores (ratings) differences compared to the last elo score calculated.\n",
    "3. Tracking the change in score across many timesteps requiring the Elo score to have the same pattern multiple times.\n",
    "4. Measuring the slope of the change across multiple entries\n",
    "\n",
    "Let's look at some timeseries to get a better sense of which method to use and how many data points we need.\n",
    "\n",
    "Before we do that, let's update the data a little bit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering  <a class=\"anchor\" id=\"feat_eng\"></a>\n",
    "\n",
    "The data is not optimal for this task yet because each row is for one game which consists of two players. We need one row per game per player.\n",
    "Here are the action items for feature engineering:\n",
    "\n",
    "- Double length of dataframe by creating individual rows per player (both white and black).\n",
    "- Create a new column when the player was white versus black because white plays first.\n",
    "- Add column showing when the time difference for when the last game was played.\n",
    "- Add column for the difference between ratings.\n",
    "\n",
    "Let's start by creating the new table focused on _players x games_ instead of only games.\n",
    "\n",
    "Note: renaming `white` and `black` to be `player` (primary) and `opponent` (secondary). Every white and every black gets a row where it is the primary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chess_white = chess.copy()\n",
    "chess_black = chess.copy()\n",
    "\n",
    "for col in chess.columns:\n",
    "    if 'black' in col:\n",
    "        chess_white.rename(columns={col: col.replace(\"black\", \"opponent\")}, inplace=True)\n",
    "        chess_black.rename(columns={col: col.replace(\"black\", \"player\")}, inplace=True)\n",
    "    if 'white' in col:\n",
    "        chess_white.rename(columns={col: col.replace(\"white\", \"player\")}, inplace=True)\n",
    "        chess_black.rename(columns={col: col.replace(\"white\", \"opponent\")}, inplace=True)\n",
    "\n",
    "# encode meaning of white versus black which encodes order\n",
    "chess_white[\"goes_first\"] = 1\n",
    "chess_black[\"goes_first\"] = 0\n",
    "\n",
    "# for black, need to reverse what outcome means\n",
    "chess_black.loc[chess_black.outcome == 0, 'outcome'] = 3 # placeholder\n",
    "chess_black.loc[chess_black.outcome == 1, 'outcome'] = 4 # placeholder\n",
    "\n",
    "chess_black.loc[chess_black.outcome == 3, 'outcome'] = 1 # where black won\n",
    "chess_black.loc[chess_black.outcome == 4, 'outcome'] = 0 # where white won\n",
    "\n",
    "chess_df = pd.concat([chess_white, chess_black])\n",
    "chess_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much time is there between games? We can see below that there is a long tail of large amounts of time between games but most of the games are played in a relatively recent time period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chess_df.sort_values(['player_username', 'timestamp'], inplace=True)\n",
    "chess_df = chess_df.reset_index(drop=True)\n",
    "\n",
    "chess_df['time_diff'] = chess_df.groupby('player_username').timestamp.transform(lambda x: (x - x.min())/60)\n",
    "chess_df[\"time_diff\"].hist() # in units of minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also look at the difference between each player's ratings for every game.\n",
    "\n",
    "We see there are a few outliers, but mostly all the scores are within 100 points of each other. This make sense given the US Chess Federation rating assigns classes every 100-200 points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chess_df[\"score_diff\"] = chess_df[\"player_rating\"] - chess_df[\"opponent_rating\"]\n",
    "\n",
    "chess_df[\"score_diff\"].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot timeseries <a class=\"anchor\" id=\"timeseries\"></a>\n",
    "\n",
    "I'm considering 4 choices to separate the classes.\n",
    "Let's get some insights from the timeseries plots.\n",
    "\n",
    "To start, let's create a dataframe to save how many games each player has done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_games = chess_df.groupby(\"player_username\").timestamp.count().reset_index().sort_values(by='timestamp', ascending=False)\n",
    "num_of_games.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The player with the most games has 100x the number of games compared to the other players!\n",
    "\n",
    "Let's first focus on players with more than 3 data points to get an understanding of what \"winning\", \"losing\", and \"maintaining\" might mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the timeseries.\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title('ieshuaganocry')\n",
    "plt.plot(chess_df.loc[chess_df.player_username == 'ieshuaganocry', 'datetime'],\n",
    "         chess_df.loc[chess_df.player_username == 'ieshuaganocry', 'player_rating'])\n",
    "plt.scatter(chess_df.loc[chess_df.player_username == 'ieshuaganocry', 'datetime'],\n",
    "            chess_df.loc[chess_df.player_username =='ieshuaganocry', 'player_rating'],\n",
    "            alpha=0.3)\n",
    "\n",
    "plt.xlabel(\"time\")\n",
    "plt.ylabel('player rating/elo score')\n",
    "\n",
    "fig, axes = plt.subplots(3, 4, figsize=(10,10))\n",
    "ax = axes.flat\n",
    "\n",
    "for i, player in enumerate(num_of_games.loc[(num_of_games.timestamp > 3) & (num_of_games.timestamp < 100), 'player_username']):\n",
    "    ax[i].set_title(player)\n",
    "    ax[i].plot(chess_df.loc[chess_df.player_username == player, 'time_diff'],\n",
    "               chess_df.loc[chess_df.player_username == player, 'player_rating'])\n",
    "\n",
    "    ax[i].scatter(chess_df.loc[chess_df.player_username == player, 'time_diff'],\n",
    "                  chess_df.loc[chess_df.player_username ==player, 'player_rating'],\n",
    "                  alpha=0.3)\n",
    "    \n",
    "ax[0].set_ylabel('elo_score')\n",
    "ax[-1].set_xlabel('time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we answer the question of what to use, I noticed in these plots that there were often repeated ratings at approximately the same time.\n",
    "(I set an `alpha` to the plots, so the darker the circles from the scatter plot, the more data points are located there.)\n",
    "\n",
    "Let's look at what this means for a high frequency player.\n",
    "We can see below that the outcome is changing, the player is winning and losing games, but the player rating is not changing in time. They are playing the same time class on the same service. Looking at the highest frequency player, I determined that the scores appear to update approximately once every 30 mins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player = 'hooligandi'\n",
    "chess_df.loc[chess_df.player_username == player, ['datetime',  'player_rating','outcome', 'opponent_rating', 'time_class']].copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go back and examine the four choices I outlines above.\n",
    "\n",
    "1. Multipe wins, loses, or draws (maintain) in a row. If so, how many?\n",
    "    * The timeseries did not address this question because I did not plot outcomes over time.\n",
    "    * However, we can see from the table example above that games are being won and lost, but the elo rating is not changing. Given that the elo score is explicitly mentioned in the prompt, I will assume that is the metric we want to track (though the outcomes are a tracer of the elo score).\n",
    "2. Postive, negative, same elo scores (ratings) differences compared to the last elo score calculated.\n",
    "    * This is viable for a one shot description, which again may not be good enough to define \"winning\"\n",
    "3. Tracking the change in score across many timesteps requiring the Elo score to have the same pattern multiple times.\n",
    "    * This has the problem of the repeated ELO scores.\n",
    "    * Why is this not the same as #1? There are repeated ratings across many games. It appears that the ratings are only updated approximately every 30 mins. If you play many games quickly in a row, the ELO score doesn't change.\n",
    "4. Measuring the slope of the change across multiple entries. Is so, how many?\n",
    "    * This is the measurement that I am going to use.\n",
    "    * This takes a wider look to see what the current trend is. It will measure if the elo rating is trending up or down so that if a player is having an overall winning streak, but loses a game, the slope may still be positive.\n",
    "    * How many points to use when calculating the slope? I chose 3 points but would prefer to use more. I used 3 points to allow in a maximum number of players where we can get a measurement of how they are performing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slope Calculation <a class=\"anchor\" id=\"slope\"></a>\n",
    "\n",
    "To calculate slope, I'm going to use:\n",
    "- rolling calulation with a minium of 3 data points. Must have 3 data points in order to calculate slope. The first two entries will not have measurements.\n",
    "- The x-axis is time; however, given that there can be long lags between games, I do not want to degrade the steepness of the slope by encorporating the difference in time (in seconds). Instead, I will assume only the ordering is set and use as the x-axis an array of length equal to the rolling factor.\n",
    "    - Note: I tested using the time difference, but the general results were equivalent.\n",
    "- I also tried using a rolling window over time, but too many games were played with a very wide gap and so many windows only had a single entry. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the slope function to apply to the group by.\n",
    "from scipy import stats\n",
    "\n",
    "def slope(input):\n",
    "    time = np.arange(len(input))\n",
    "    #time = chess_df.loc[df.index, 'time_diff'].to_numpy() # Example of how to use time difference in the calculation\n",
    "\n",
    "    elo_scores = input\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(time, \n",
    "                                                                   elo_scores)\n",
    "    return round(slope, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a slope column using 3 data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chess_df['elo_slope'] = chess_df.groupby(['player_username']).player_rating.rolling(3).apply(slope, raw=False).reset_index().player_rating\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the timeseries and the slope as a function of time for a few players. The red line is a zero slope, so all above zeros is \"winning\", below zeros is \"losing\", and on the line is \"maintaining\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for player in num_of_games.loc[(num_of_games.timestamp > 3), 'player_username'][:4]:\n",
    "        if player == 'ieshuaganocry':\n",
    "            figsize=(10,10)\n",
    "            time= 'datetime'\n",
    "        else:\n",
    "            figsize=(6,6)\n",
    "            time = 'time_diff'\n",
    "\n",
    "        fig, axes = plt.subplots(2, 1, figsize=figsize, sharex=True)\n",
    "        axes[0].set_title(player)\n",
    "\n",
    "        # rating plot\n",
    "        axes[0].plot(chess_df.loc[chess_df.player_username == player, time],\n",
    "                     chess_df.loc[chess_df.player_username == player, 'player_rating'],\n",
    "                     label='player rating')\n",
    "        axes[0].scatter(chess_df.loc[chess_df.player_username == player, time],\n",
    "                        chess_df.loc[chess_df.player_username == player, 'player_rating'],\n",
    "                        alpha=0.3)\n",
    "        axes[0].legend()\n",
    "\n",
    "        # Slope plot\n",
    "        axes[1].plot(chess_df.loc[chess_df.player_username == player, time],\n",
    "                     chess_df.loc[chess_df.player_username == player, 'elo_slope'])\n",
    "        axes[1].scatter(chess_df.loc[chess_df.player_username == player, time],\n",
    "                        chess_df.loc[chess_df.player_username == player, 'elo_slope'],\n",
    "                        label='slope',\n",
    "                        alpha=0.3)\n",
    "        axes[1].axhline(0, color='r', label='zero slope')\n",
    "        axes[1].legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the slope is not a perfect indicator with only 3 data points. For example `hooligandi` maintains that spike to the \"winning\" side, but they are in a losing trend. This is happening because there are several data points with the same Elo score that the rolling, 3 data point nature of the calculation captures. The way to dampen this trend would be to include more data points, but including more data points would decrease the number of players used in the behavioral analysis. \n",
    "\n",
    "I will keep the slope as is for now, but note that a longer term analysis needs more data to avoid these issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Side bar: Elo score: Can I calculate it? <a class=\"anchor\" id=\"calculate-elo\"></a>\n",
    "\n",
    "Looking at the timeseries of the elo score, it appears that the score is not actually being updated after every game. Looking at the formula from Wikipedia, I want to check if I can update the elo score to actually update and match the score when it changes. That way the results from the slope will be more accurate.\n",
    "\n",
    "Function is rating of opponents plus 400 per win, minus 400 per loss, and plus nothing if draw. Then divided by the number of games played.\n",
    "\n",
    "As we can see below, I am _not_ able to reproduce the elo scores using a subset. I will have to continue with the data set as is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## tested using the elo formula\n",
    "player = 'hooligandi'\n",
    "check = chess_df.loc[chess_df.player_username == player, ['datetime',  'time_diff', 'player_rating','outcome', 'opponent_rating', 'time_class', 'elo_slope']].copy()\n",
    "\n",
    "check['adjustment'] = 0\n",
    "check.loc[check.outcome == 0, 'adjustment'] = -400\n",
    "check.loc[check.outcome == 1, 'adjustment'] = 400\n",
    "check[\"elo_update\"] = (check[\"player_rating\"] + check[\"opponent_rating\"] + check['adjustment'])/2.\n",
    "check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our separations!\n",
    "\n",
    "- Winning occurs when `elo_slope` > 0\n",
    "- Losing occurs when `elo_slope` < 0\n",
    "- Maintaining occurs when `elo_slope` = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different behaviors of different groups <a class=\"anchor\" id=\"behavior\"></a>\n",
    "\n",
    "Now that the classes have been defined, let's examine how the behavior is different.\n",
    "\n",
    "1. Histograms, medians, and means.\n",
    "2. Common games across the classes\n",
    "3. How do errors evolve over time within the game\n",
    "4. Difference between Time Classes\n",
    "5. Are the different behaviors statistically significant\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate out into different data frames and check length to understand scope. Winning and losing have about the same number of entries, while maintainig is the smallest set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winning = chess_df[chess_df.elo_slope > 0].copy()\n",
    "losing = chess_df[chess_df.elo_slope < 0].copy()\n",
    "maintain = chess_df[chess_df.elo_slope == 0].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('length winning:', len(winning))\n",
    "print('length losing:', len(losing))\n",
    "print('length maintain:', len(maintain))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histograms, medians, and means <a class=\"anchor\" id=\"hist\"></a>\n",
    "\n",
    "The question asks how does a _player_'s behavior change, so I'm going to focus on the primary player and not consider what the opponent is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_columns = []\n",
    "opponent_columns = []\n",
    "all_other = ['goes_first', 'score_diff', 'time_class_encoded',\n",
    "             'eco_name_encoded', 'eco_code_encoded', 'eco_category_encoded']\n",
    "for c in winning.columns:\n",
    "    if 'player' in c and 'username' not in c:\n",
    "        player_columns.append(c)\n",
    "    if 'opponent' in c and 'username' not in c:\n",
    "        opponent_columns.append(c)\n",
    "\n",
    "print(len(player_columns), len(opponent_columns), len(all_other))\n",
    "# Only want PLAYER difference, so don't need to look at opponent columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create histograms of all the features for winning, losing, and maintain. I'm also plotting the median of each distribution with the actual value printed in the legend.\n",
    "\n",
    "To have fewer objects in this plot, I've separated out the columns by player columns and other interesting information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_dict = {}  # Save mean, median, standard deviation\n",
    "fig, axes = plt.subplots(6, 5, figsize=(22,22))\n",
    "ax = axes.flat\n",
    "\n",
    "for i, c in enumerate(player_columns):\n",
    "    # Plot the histogram per winning, losing, maintain. \n",
    "    # Want these to be directly comparable, so they need the same binning.\n",
    "    _max = max(winning[c].max(), losing[c].max(), maintain[c].max())\n",
    "    ax[i].hist(winning[c], density=True, bins=np.linspace(0, _max, 15),\n",
    "         alpha=0.5)\n",
    "    ax[i].hist(losing[c], density=True, bins=np.linspace(0, _max, 15),\n",
    "            alpha=0.4)\n",
    "    ax[i].hist(maintain[c], density=True, bins=np.linspace(0, _max, 15),\n",
    "            alpha=0.3)\n",
    "\n",
    "    # Calculate median and save other interesting metrics.\n",
    "    win_med = winning[c].median()\n",
    "    los_med = losing[c].median()\n",
    "    mai_med = maintain[c].median()\n",
    "\n",
    "    stats_dict[c] = {'winning_median': win_med, 'losing_median': los_med, \"maintain_median\": los_med,\n",
    "                     'winning_mean': winning[c].mean(), 'losing_mean': losing[c].mean(), \"maintain_mean\": maintain[c].mean(),\n",
    "                     'winning_std': winning[c].std(), 'losing_std': losing[c].std(), \"maintain_std\": maintain[c].std()}\n",
    "\n",
    "    # Add vertical line at median.\n",
    "    ax[i].axvline(win_med, color='#1f77b4', label=f'Winning: {round(win_med, 2)}')\n",
    "    ax[i].axvline(los_med, color='#ff7f0e', label=f'Losing: {round(los_med, 2)}')\n",
    "    ax[i].axvline(mai_med, color='#2ca02c', label=f'Maintaining: {round(mai_med, 2)}')\n",
    "\n",
    "    ax[i].legend()\n",
    "    ax[i].set_xlabel(c[7:])  # removing \"player\" from label to make easier to read\n",
    "    \n",
    "ax[0].set_ylabel('probability density')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(10, 7))\n",
    "ax = axes.flat\n",
    "\n",
    "for i, c in enumerate(all_other):\n",
    "    # Plot the histogram per winning, losing, maintain. \n",
    "    # Want these to be directly comparable, so they need the same binning.\n",
    "    _max = max(winning[c].max(), losing[c].max(), maintain[c].max())\n",
    "    ax[i].hist(winning[c], density=True, bins=np.linspace(0, _max, 15),\n",
    "         alpha=0.5)\n",
    "    ax[i].hist(losing[c], density=True, bins=np.linspace(0, _max, 15),\n",
    "            alpha=0.4)\n",
    "    ax[i].hist(maintain[c], density=True, bins=np.linspace(0, _max, 15),\n",
    "            alpha=0.3)\n",
    "\n",
    "    # Calculate median and save other interesting metrics.\n",
    "    win_med = winning[c].median()\n",
    "    los_med = losing[c].median()\n",
    "    mai_med = maintain[c].median()\n",
    "\n",
    "    stats_dict[c] = {'winning_median': win_med, 'losing_median': los_med, \"maintain_median\": los_med,\n",
    "                       'winning_mean': winning[c].mean(), 'losing_mean': losing[c].mean(), \"maintain_mean\": maintain[c].mean(),\n",
    "                       'winning_std': winning[c].std(), 'losing_std': losing[c].std(), \"maintain_std\": maintain[c].std()}\n",
    "\n",
    "    # Add vertical line at median.\n",
    "    ax[i].axvline(win_med, color='#1f77b4', label=f'Winning: {round(win_med, 2)}')\n",
    "    ax[i].axvline(los_med, color='#ff7f0e', label=f'Losing: {round(los_med, 2)}')\n",
    "    ax[i].axvline(mai_med, color='#2ca02c', label=f'Maintaining: {round(mai_med, 2)}')\n",
    "\n",
    "    ax[i].legend()\n",
    "    ax[i].set_xlabel(c)\n",
    "    \n",
    "ax[0].set_ylabel('probability density')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I recorded the mean, median, and standard deviation for each distribution to look a little closer at the numbers.\n",
    "\n",
    "Note: when mean does not equal median, the distribution is not Gaussian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df = pd.DataFrame(stats_dict)\n",
    "stats_df.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of histogram, means, medians seen above:\n",
    "\n",
    "- `Opening Score`: Winning group plays a better opening move than maintaining or losing.\n",
    "    - Options = [0, 1] (int)\n",
    "    - winning has more 1s than zeros > maintaining > losing has more zeros than 1s\n",
    "- `Advantage Capitalization Winrate`: Measures how well a player can turn early advantages into wins\n",
    "    - Winners have an average of 2x better winrate over losing elo scores. Gap is much smaller comparing Winning to Maintaining.\n",
    "    - Options = [0, 1] (int)\n",
    "    - winning has more 1s than zeros > maintaining > losing has more zeros than 1s\n",
    "- `Resourcefullness Win draw rate`: The ability to win or draw when at a disadvantage\n",
    "    - Winning behavior has a much better ability to win or draw at disadvange. The average score for winning is 2x Maintiaing and >4x losing. \n",
    "    - Options = [0, 1] (int)\n",
    "    - winning has more 1s than zeros > maintaining > losing has more zeros than 1s\n",
    "- `Tactics Score`: Percent assigned based on performance.\n",
    "    - Float percent, range 0-1\n",
    "    - Winners on average have 18% higher score compared to losing, but only a 5% higher score compared to maintaining\n",
    "    - winning > maintaining > losing\n",
    "- `Inaccuracies per game`: Range from 0-1. Winning shows slightly fewer inaccuracies per game with winning < maintaining < losing.\n",
    "    - `Opening`: maintaining games have more inaccuraies in openings than losing! winning < losing < maintaining\n",
    "    - `Middle`: But this is reversed back to expected by the middle of the game winning < maintaining < losing\n",
    "    - `End`: The difference between the groups widens by the time we get to the end\n",
    "- `Mistakes per game`: Range from 0-1. Winning shows fewer mistakes per game with winning < maintaining < losing\n",
    "    - `Opening`: We see the same issues as above where the maintaing make more mistakes in opening. winning < losing < maintaining\n",
    "    - `Middle`: But this is fixed by midgame. winning < maintaining < losing\n",
    "    - `End`: Continue the expected trend at the end. winning < maintaining < losing\n",
    "- `Blunders per game`: Range from 0-1. The losing group has a fairly wide difference between winning here, almost 2x. winning < maintaining < losing\n",
    "    - `Opening`: Winning group is doing much better but again we see maintaining performing slightly worse. winning < losing < maintaining\n",
    "    - `Middle`: But maintiaining is close to the winning group by endgame. winning < maintaining < losing\n",
    "    - `End`: Blunder rate for losing is 3x the rate for winning team while maintaining is much closer. winning < maintaining < losing\n",
    "- `Endgame Win Rate`: Options = [0, 1] (int), There are very few wins for the losing group (as expected). The maintaining group is similar to winning but winning is better.\n",
    "    - `Endgame Win rate with equal`: Same as above. \n",
    "    - `Endgame Win rate with advantage`: Same as above but winning and maintaining are 2x higher than losing.\n",
    "    - `Endgame Win rate with disadvantage`: The winning group has the biggest difference here. They perform 2x better than maintaining and almost 10x better than losing.\n",
    "- `Long thinking outcome score`\n",
    "    - Float percent, range 0-1\n",
    "    - This is a fairly flat distribution but we do see winning > maintaining > losing. Winning tends to have better long thinking outcome scorees.\n",
    "- `Time advantage score`: Weaker player receives more time to think. Winning receives the smallest advantage and maintaining receives the largest advantage. Float percent\n",
    "    - `Significant Time advantage win rate`: Options = [0, 1] (int) if they received the advantage and won. If in the winning group, it is much more likely that you will win versus the other groups. Nearly 3x compared to the losing group.\n",
    "    - `Significant Time disadvantage win rate`:  Options = [0, 1] (int), if they received disadvantage and won.The winning group is less likley to convert these games to win, but they still have a major advantage over the losing groups. The winning group can convert this about 5x more than th elosing group.\n",
    "\n",
    "\n",
    "\n",
    "Extra notes:\n",
    "- Going first gives an advantage to win, but it might be a small advantage. More rigourous statistical studies required.\n",
    "- A player with a higher Elo score is more likely to win.\n",
    "- player ACL = average centipawn loss . As the score decreases, your accuracy increases. It is interesting that the losing trend has a lower ACL than the winning trend.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common opening moves across the classes <a class=\"anchor\" id=\"eco\"></a>\n",
    "\n",
    "Given the nature of the Eco groups above, this way of looking at the data may not be helpful. So let's look at a different way.\n",
    "\n",
    "(These plots are messy and I would not show them to a customer, but they are to give me an understanding of what is going on.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eco\n",
    "for col in ['eco_category', 'eco_code', 'eco_name']: # in increasing complexity\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(15, 10), sharex=True) #, sharex=True)\n",
    "    # note player_username is just a placeholder\n",
    "    winning.groupby([col]).player_username.count().reset_index().plot.barh(x=col, y='player_username', ax=ax[0], label='winning')\n",
    "    losing.groupby([col]).player_username.count().reset_index().plot.barh(x=col, y='player_username', ax=ax[1], color='#ff7f0e', label='losing')\n",
    "    maintain.groupby([col]).player_username.count().reset_index().plot.barh(x=col, y='player_username', ax=ax[2], color='#2ca02c', label='maintain')\n",
    "\n",
    "    # use same x limit across all subplots\n",
    "    ax[0].set_xlim(0, max(winning.groupby([col]).player_username.count().reset_index().player_username.max(),\n",
    "                          losing.groupby([col]).player_username.count().reset_index().player_username.max(),\n",
    "                          maintain.groupby([col]).player_username.count().reset_index().player_username.max()\n",
    "                          ))\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eco Classification Summary\n",
    "\n",
    "- `eco_category`\n",
    "    - Winning opening moves/games played:\n",
    "        1. Russian Game.\n",
    "        2. Sicilian Defense\n",
    "        3. Queen's Pawn Game\n",
    "        4. French Defense\n",
    "    - Losing opening moves/games played:\n",
    "        1. Russian Game.\n",
    "        2. Queen's Pawn Game\n",
    "        3. Sicilian Defense\n",
    "        4. The Queen's Gambit Refused\n",
    "    - Maintain opening moves/games played:\n",
    "        1. Russian Game.\n",
    "        2. Sicilian Defense\n",
    "        3. French Defense\n",
    "        4. Four Knights Game\n",
    "- `eco_code`\n",
    "    - C42 is the Russian Game\n",
    "    - Which one of these you use is dependent on how granular you need the information. \n",
    "- `eco_name`\n",
    "    - Most granulary specification for game classification and as such we see the flattest distribution. Though a few specific games have up to 16 plays, the majority are a single play per group.\n",
    "\n",
    "Everyone is playing the same opening moves the most often across all groups. Another interesting thing to explore would be the error and win rates per different opening moves. However, that is out of scope for this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do errors evolve over time within a game <a class=\"anchor\" id=\"gameplay\"></a>\n",
    "\n",
    "Relative to the winning numbers, is the losing group or maintaining group changing in mistakes. I am not looking at the absolute change because I want to understand how they are different from the winning team."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exploring different times in the game play\n",
    "over_time = {}\n",
    "i = 0\n",
    "for type_ in ['inaccuracies', 'mistakes', 'blunders']:\n",
    "    for game in ['opening', 'middlegame', 'endgame']:\n",
    "        over_time[i] = {\"issue\": type_, \"time\": game,\n",
    "                        \"losing\": stats_df[f'player_{type_}_per_{game}'].iloc[4]/stats_df[f'player_{type_}_per_{game}'].iloc[3],\n",
    "                        \"maintain\": stats_df[f'player_{type_}_per_{game}'].iloc[5]/stats_df[f'player_{type_}_per_{game}'].iloc[3]}\n",
    "        i += 1\n",
    "\n",
    "change_v_winning = pd.DataFrame(over_time).T\n",
    "change_v_winning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Evolution Summary\n",
    "\n",
    "- Losing:\n",
    "    - inaccuracies increase with time\n",
    "    - mistakes increase with time\n",
    "    - blunders decrease in middle game but then increase\n",
    "- Maintain:\n",
    "    - Start out with the highest amount of mistakes, then perform fairly close to the winning team in the middle game, but finish with increased number of mistakes, typically less than the start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Difference Between Time Classes <a class=\"anchor\" id=\"timeclass\"></a>\n",
    "\n",
    "Are the distributions different across the different kinds of games people are playing?\n",
    "\n",
    "Do we have enough data to answer this question?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different in winning versus in different versions of game\n",
    "\n",
    "print('All', chess_df.groupby('time_class').outcome.count(), '\\n')\n",
    "print('Winning', winning.groupby('time_class').outcome.count(), '\\n')\n",
    "print('Losing', losing.groupby('time_class').outcome.count(), '\\n')\n",
    "print('Maintain', maintain.groupby('time_class').outcome.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check to see if there are differences in game play for winning because it has the most entries in 2 of the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Winning\n",
    "time_class_dict = {}\n",
    "fig, axes = plt.subplots(6, 5, figsize=(22,22))\n",
    "ax = axes.flat\n",
    "\n",
    "for i, c in enumerate(player_columns):\n",
    "    for tc, color in zip(winning.time_class.unique(), ['#1f77b4', '#ff7f0e', '#2ca02c','#9467bd']):\n",
    "        ax[i].hist(winning.loc[winning.time_class == tc, c], density=True, bins=np.linspace(0, winning[c].max(), 15),\n",
    "             alpha=0.5)\n",
    "        time_class_dict[c] = {f'{tc}_median': winning.loc[winning.time_class == tc, c].median(),\n",
    "                              f'{tc}_mean': winning.loc[winning.time_class == tc, c].mean(),\n",
    "                              f'{tc}_std': winning.loc[winning.time_class == tc, c].std()}\n",
    "\n",
    "        label_num = round(time_class_dict[c][f'{tc}_median'], 2)\n",
    "        ax[i].axvline(time_class_dict[c][f'{tc}_median'], color=color, label=f'{tc}: {label_num}') # {round(time_class_dict[c]['{tc}_median'], 2)}\n",
    "\n",
    "    ax[i].legend()\n",
    "    ax[i].set_xlabel(c[7:])\n",
    "    \n",
    "ax[0].set_ylabel('probability density')\n",
    "\n",
    "plt.suptitle('Winning')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Losing\n",
    "time_class_dict = {}\n",
    "fig, axes = plt.subplots(6, 5, figsize=(22,22))\n",
    "ax = axes.flat\n",
    "\n",
    "for i, c in enumerate(player_columns):\n",
    "    for tc, color in zip(losing.time_class.unique(), ['#1f77b4', '#ff7f0e', '#2ca02c','#9467bd']):\n",
    "        ax[i].hist(losing.loc[losing.time_class == tc, c], density=True, bins=np.linspace(0, winning[c].max(), 15),\n",
    "             alpha=0.5)\n",
    "        time_class_dict[c] = {f'{tc}_median': losing.loc[losing.time_class == tc, c].median(),\n",
    "                              f'{tc}_mean': losing.loc[losing.time_class == tc, c].mean(),\n",
    "                              f'{tc}_std': losing.loc[losing.time_class == tc, c].std()}\n",
    "\n",
    "        label_num = round(time_class_dict[c][f'{tc}_median'], 2)\n",
    "        ax[i].axvline(time_class_dict[c][f'{tc}_median'], color=color, label=f'{tc}: {label_num}') # {round(time_class_dict[c]['{tc}_median'], 2)}\n",
    "\n",
    "    ax[i].legend()\n",
    "    ax[i].set_xlabel(c[7:])\n",
    "    \n",
    "ax[0].set_ylabel('probability density')\n",
    "\n",
    "plt.suptitle('Losing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Class Summary\n",
    "\n",
    "The winning group showed little difference between the time class of games and the losing group showed a higher difference between them. However, most of the games are `blitz` and the losing group has even less samples. The differences are likely due to low number statistics in the other games though I would expect _strategies_ to be different across different time games. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Tests <a class=\"anchor\" id=\"hypothesis\"></a>\n",
    "Are these really 3 individual samples?\n",
    "\n",
    "I will run 3 tests:\n",
    "- ANOVA to test if all three samples come from the same distribution\n",
    "- 2 sample z-test to see if each pair come from the same distribution for float columns\n",
    "- 2 sample proportion test to see if each pair come from the same distribution for binary columns\n",
    "\n",
    "Null hypopthesis in all cases is that they come from same distribution and p-value < 0.05 is significant enough to reject the Null hypothesis. I will run this over all columns.\n",
    "\n",
    "Note: \n",
    "- I will not do this for the different time classes. There is not enough data across the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define which columns use binary designations and so need the Proportion test instead of the z-test\n",
    "binary_columns = ['player_opening_score', 'player_advantage_capitalization_winrate', 'player_resourcefulness_windrawrate',\n",
    "                  'player_endgame_winrate',\n",
    "                  'player_endgame_winrate_with_equal',\n",
    "                  'player_endgame_winrate_with_advantage',\n",
    "                  'player_endgame_winrate_with_disadvantage',\n",
    "                  'player_significant_time_advantage_winrate',\n",
    "                  'player_significant_time_disadvantage_winrate',\n",
    "                  'goes_first']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANOVA test across the 3 samples. \n",
    "This implementation returns the F score and p-value. I am only surfacing the p-value to get a sense of the numbers. I have also flagged when the Null Hypothesis is not rejected (`SAME DISTRIBUTION`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the list of columns for the z-test\n",
    "cols_for_float_test = list(set(chess_df.columns) - set(binary_columns) - set(['datetime', 'timestamp', 'time_diff', 'outcome', 'player_username', 'time_class_encoded']) - set(categorical_columns))\n",
    "\n",
    "# ANOVA Test for all float statistics. \n",
    "from scipy.stats import f_oneway\n",
    "\n",
    "for fcol in cols_for_float_test:\n",
    "    if 'opponent' not in fcol and 'eco' not in fcol:\n",
    "        fstat, pvalue = f_oneway(winning[fcol], losing[fcol], maintain[fcol], nan_policy='omit')\n",
    "        if pvalue > 0.05:\n",
    "            print(f'SAME DISTRIBUTION: {fcol} pvalue={round(pvalue, 4)}')\n",
    "        else:\n",
    "            print(f'{fcol} pvalue={round(pvalue, 4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Sample z-test\n",
    "This will see if any of the two distribuions are similar.\n",
    "- Winning versus Losing\n",
    "- Winning versus Maintain\n",
    "- Losing versus Maintain\n",
    "\n",
    "For float columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels as sm\n",
    "\n",
    "# Winning versus Losing\n",
    "same = 0\n",
    "different = 0\n",
    "for fcol in cols_for_float_test:\n",
    "    if 'player' in fcol:\n",
    "        zstat, pvalue = sm.stats.weightstats.ztest(winning[fcol].dropna(), losing[fcol].dropna(), value=0)\n",
    "        if pvalue > 0.05:\n",
    "            #print(f'SAME DISTRIBUTION: {fcol} pvalue={round(pvalue, 4)}')\n",
    "            same += 1\n",
    "        else:\n",
    "            #print(f'{fcol} pvalue={round(pvalue, 4)}')\n",
    "            different += 1\n",
    "print(f'Between Winning and Losing, {same} columns have same distribution, {different} columns reject the Null Hypothesis')\n",
    "\n",
    "# Winning versus Maintaining\n",
    "same = 0\n",
    "different = 0\n",
    "win_maintain_diff = []\n",
    "for fcol in cols_for_float_test:\n",
    "    if 'player' in fcol:\n",
    "        zstat, pvalue = sm.stats.weightstats.ztest(winning[fcol].dropna(), maintain[fcol].dropna(), value=0)\n",
    "        if pvalue > 0.05:\n",
    "            #print(f'SAME DISTRIBUTION: {fcol} pvalue={round(pvalue, 4)}')\n",
    "            win_maintain_diff.append(fcol)\n",
    "            same += 1\n",
    "        else:\n",
    "            #print(f'{fcol} pvalue={round(pvalue, 4)}')\n",
    "            different += 1\n",
    "print(f'Between Winning and Maintain, {same} columns have same distribution, {different} columns reject the Null Hypothesis')\n",
    "\n",
    "\n",
    "# Losing versus Maintaining\n",
    "same = 0\n",
    "different = 0\n",
    "lose_maintain_diff = []\n",
    "for fcol in cols_for_float_test:\n",
    "    if 'player' in fcol:\n",
    "        zstat, pvalue = sm.stats.weightstats.ztest(maintain[fcol].dropna(), losing[fcol].dropna(), value=0)\n",
    "        if pvalue > 0.05:\n",
    "            #print(f'SAME DISTRIBUTION: {fcol} pvalue={round(pvalue, 4)}')\n",
    "            lose_maintain_diff.append(fcol)\n",
    "            same += 1\n",
    "        else:\n",
    "            #print(f'{fcol} pvalue={round(pvalue, 4)}')\n",
    "            different += 1\n",
    "print(f'Between Maintain and Losing, {same} columns have same distribution, {different} columns reject the Null Hypothesis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Win/Maintain Diff', win_maintain_diff, '\\n')\n",
    "print('Lose/Maintain Diff', lose_maintain_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Sample Proportion test test\n",
    "This will see if any of the two distribuions are similar.\n",
    "- Winning versus Losing\n",
    "- Winning versus Maintain\n",
    "- Losing versus Maintain\n",
    "\n",
    "For binary columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Winning versus Losing\n",
    "same = 0\n",
    "different = 0\n",
    "for col in binary_columns:\n",
    "    ncount = np.array([winning[col].sum(), losing[col].sum()])\n",
    "    nobs = np.array([winning[col].count(), losing[col].count()])\n",
    "\n",
    "    stat, pval = sm.stats.proportion.proportions_ztest(ncount, nobs)\n",
    "    if pval > 0.05:\n",
    "        #print(f'SAME DISTRIBUTION: {col} pvalue={round(pval, 4)}')\n",
    "        same += 1\n",
    "    else:\n",
    "        #print(f'{col} pvalue={round(pval, 4)}')\n",
    "        different += 1\n",
    "    \n",
    "print(f'Between Winning and Losing, {same} columns have same distribution, {different} columns reject the Null Hypothesis')\n",
    "\n",
    "# Winning versus Maintaining\n",
    "same = 0\n",
    "different = 0\n",
    "for col in binary_columns:\n",
    "    ncount = np.array([winning[col].sum(), maintain[col].sum()])\n",
    "    nobs = np.array([winning[col].count(), maintain[col].count()])\n",
    "\n",
    "    stat, pval = sm.stats.proportion.proportions_ztest(ncount, nobs)\n",
    "    if pval > 0.05:\n",
    "        #print(f'SAME DISTRIBUTION: {col} pvalue={round(pval, 4)}')\n",
    "        same += 1\n",
    "    else:\n",
    "        #print(f'{col} pvalue={round(pval, 4)}')\n",
    "        different += 1\n",
    "    \n",
    "print(f'Between Winning and Maintain, {same} columns have same distribution, {different} columns reject the Null Hypothesis')\n",
    "\n",
    "# Losing versus Maintaining\n",
    "same = 0\n",
    "different = 0\n",
    "for col in binary_columns:\n",
    "    ncount = np.array([losing[col].sum(), maintain[col].sum()])\n",
    "    nobs = np.array([losing[col].count(), maintain[col].count()])\n",
    "\n",
    "    stat, pval = sm.stats.proportion.proportions_ztest(ncount, nobs)\n",
    "    if pval > 0.05:\n",
    "        #print(f'SAME DISTRIBUTION: {col} pvalue={round(pval, 4)}')\n",
    "        same += 1\n",
    "    else:\n",
    "        #print(f'{col} pvalue={round(pval, 4)}')\n",
    "        different += 1\n",
    "    \n",
    "print(f'Between Losing and Maintain, {same} columns have same distribution, {different} columns reject the Null Hypothesis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis/Statistics Summary\n",
    "\n",
    "- The majority of the statistics comparing all 3 groups (winning, losing, maintain) reject the null hypothesis except `mistakes_per_opening`, `inaccuracies_per_opening`, and `time_advantage_score`. \n",
    "- Winning and Losing are the most distinct from each other having the largest number of columns the reject the null hypothesis.\n",
    "    - The null hypothesis is not rejected for the opening score and which player opened.\n",
    "    - This is not surprising because we see that the different groups used the same opening moves and black versus white is assigned randomly.\n",
    "- Winning and Maintaining mostly come from the same distributions, except 10 columns. \n",
    "    - The null hypothesis is not rejected for inaccuracies, mistakes, middle game blunders, endgame winrates, opening score, endgame winrates, which player opened.\n",
    "- Maintain and Losing are less similar than Winning and maintaining, but are more similar than winning and losing\n",
    "    - The null hypothesis is not rejected for acl, mistakes, blunders in opening, opening score, which player opened.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of Results <a class=\"anchor\" id=\"results\"></a>\n",
    "- The winning group plays strong opening moves and has relatively fewer mistakes, inaccuracies, and blunders. \n",
    "- The losing group is typically weak all around creating more mistakes, inaccuracies, and blunders\n",
    "    - Typically have more errors (inaccuracies, mistakes, blunders) as the game continues.\n",
    "- Maintaining group is interesting. They have more mistakes, inaccuracies, and blunders when starting the game, but recover to be close to the winning group by midgame. My conclusion is that they had great potential of winning the game if they had a stronger start.\n",
    "    - This group has the most errors at the beginning of the game, the fewest errors in the middle of the game, and then start having more errors towards the end (though never matching errors at the start of the game.)\n",
    "- Every group is playing the same classifications of opening moves\n",
    "\n",
    "Though these groups do have some similarities, they do broadly represent different playing distributions.\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: ML Classification <a class=\"anchor\" id=\"bonus\"></a>\n",
    "\n",
    "We have defined \"winning\" (positive slope), \"losing\" (negative slope), and \"maintain\" (zero slope) classes.\n",
    "We have also seen that the metrics from game play are generally distinct groups.\n",
    "Now that we have these labels and this information, we can try running a classifier to see when people are in different groups.\n",
    "\n",
    "What steps do we need to take:\n",
    "1. Preprocess Data:\n",
    "    - One hot encode catgoricals\n",
    "    - Null Handling\n",
    "2. Create a train/test split\n",
    "3. Standardize data\n",
    "4. Train a model\n",
    "5. Evaluate performance\n",
    "\n",
    "\n",
    "### Step 1: Preprocess Data\n",
    "\n",
    "The categorical columns do not actually represent a meaning order. We could rearrange them and it would not matter. Therefore, ordinal encoding is creating information that does not exist. One way to remove that information while still encoding categoricals is to use one-hot encoding. One hot encoding will create new columns per category and the entries will be zero or one depending on if that row has the categorical.\n",
    "The disadvantage is that we are introducing many sparse columns to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Ordinally encoded categoricals. Will replace with one-hot\n",
    "chess_df_enc = chess_df.drop(columns=encoded_names)\n",
    "\n",
    "chess_df_enc = pd.get_dummies(chess_df_enc, columns=[\"time_class\", \"eco_name\", \"eco_code\", \"eco_category\", \"service\"],  dtype=float)\n",
    "chess_df_enc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop any entry that does not have an `elo_slope` because we cannot classify it.\n",
    "\n",
    "Fill remaining NaNs with zeros so the ML works properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chess_df_enc = chess_df_enc[~chess_df_enc.elo_slope.isna()].copy()\n",
    "\n",
    "# fill remaining nulls with zero\n",
    "chess_df_enc.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the target column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also need to change `elo_slope` to classification\n",
    "# 0 = winning\n",
    "# 1 = losing\n",
    "# 2 = maintain\n",
    "chess_df_enc['classification'] = 0\n",
    "chess_df_enc.loc[chess_df_enc.elo_slope < 0, 'classification'] = 1\n",
    "chess_df_enc.loc[chess_df_enc.elo_slope == 0, 'classification'] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unneeded columns\n",
    "chess_df_enc.drop(columns=['timestamp', 'player_username', 'opponent_username', 'time_diff', 'datetime', 'elo_slope'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create a train/test split\n",
    "\n",
    "Now that we've gone through data processing, can create data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "features = list(chess_df_enc.columns)\n",
    "features.remove('classification')\n",
    "\n",
    "# split data into features and labels\n",
    "X = chess_df_enc[features].copy()\n",
    "y = chess_df_enc[\"classification\"].copy()\n",
    "\n",
    "\n",
    "# Split data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    train_size=.7, # train size is 70%\n",
    "                                                    random_state=25) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Standardize data\n",
    "\n",
    "Must run standardization after performing the train/test split or else you will introduce a data leak into the standardization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Instantiate the scaler and fit on features\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Transform features\n",
    "X_train_scaled = scaler.transform(X_train.values)\n",
    "X_test_scaled = scaler.transform(X_test.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Train a Model\n",
    "\n",
    "This is a lot of tabular data, will test out two options:\n",
    "1. Logistic Regression\n",
    "2. Grdient Boosted Decision Tree Classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Instantiating the models \n",
    "logistic_regression = LogisticRegression(max_iter=500)\n",
    "tree = GradientBoostingClassifier(n_estimators=700, subsample=0.7, learning_rate=0.01, max_depth=3) # criterion, splitter, max_depth\n",
    "\n",
    "# Training the models \n",
    "logistic_regression.fit(X_train_scaled, y_train)\n",
    "tree.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Making predictions with each model\n",
    "log_reg_preds = logistic_regression.predict(X_test_scaled)\n",
    "tree_preds = tree.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Evaluate Performance\n",
    "\n",
    "Will create a classification report measuring precision, recall, f1-score, and accuracy as well as a confusion matrix. There are many other ways to measure performance, but this is not the main goal of this exercise, so I will limit evaluations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Store model predictions in a dictionary\n",
    "# this makes it's easier to iterate through each model\n",
    "# and print the results. \n",
    "model_preds = {\n",
    "    \"Logistic Regression\": log_reg_preds,\n",
    "    \"Boosted Decision Tree\": tree_preds,\n",
    "}\n",
    "\n",
    "for model, preds in model_preds.items():\n",
    "    print(f\"{model} Results:\\n{classification_report(y_test, preds)}\", sep=\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Boosted Decision Tree is outperforming the logistic regression. The logistic regression performs best with linear correlations, so maybe a non-linear model would be a better choice given this data set.\n",
    "\n",
    "Create the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# confusion matrix per model\n",
    "cnf_matrix_log = metrics.confusion_matrix(y_test, log_reg_preds)\n",
    "cnf_matrix_tree = metrics.confusion_matrix(y_test, tree_preds)\n",
    "\n",
    "class_names=[\"winning\",\"losing\", \"maintaining\"]\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix_log, columns=class_names, index=class_names), cmap=\"YlGnBu\", annot=True, ax=ax[0],fmt='g')\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix_tree, columns=class_names, index=class_names), cmap=\"YlGnBu\", annot=True, ax=ax[1],fmt='g')\n",
    "\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n",
    "\n",
    "ax[0].set_title('Logistic Regression')\n",
    "ax[1].set_title('Boosted Decision Tree')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see there is a lot of confusion around the `maintain` class. The majority of these labels are misclassified. Winning and losing are more easily separable.\n",
    "\n",
    "This quick classification exercise did not yeild good results! The BDT classifier did perform better than the logistic regression, but it's overall acurracy was very low. \n",
    "\n",
    "One other quick question:\n",
    "- Everything above ignored the opponent's information except for these classifiers. Let's do two more models ignoring the opponent's info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unwanted features\n",
    "feature_list_no_opp = list(X_train.columns)\n",
    "for c in feature_list_no_opp:\n",
    "    if 'opponent' in c:\n",
    "        feature_list_no_opp.remove(c)\n",
    "\n",
    "# Need to rescale the data, but will leave the train/test split unchanged.\n",
    "# Instantiate the scaler and fit on features\n",
    "scaler_no_opp = StandardScaler()\n",
    "scaler_no_opp.fit(X_train[feature_list_no_opp])\n",
    "        \n",
    "# Transform features\n",
    "X_train_scaled_no_opp = scaler_no_opp.transform(X_train[feature_list_no_opp].values)\n",
    "X_test_scaled_no_opp = scaler_no_opp.transform(X_test[feature_list_no_opp].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train new models\n",
    "\n",
    "# Instantiating the models \n",
    "logistic_regression_no_opp = LogisticRegression(max_iter=500)\n",
    "tree_no_opp = GradientBoostingClassifier(n_estimators=700, subsample=0.7, learning_rate=0.01, max_depth=3) # criterion, splitter, max_depth\n",
    "\n",
    "# Training the models \n",
    "logistic_regression.fit(X_train_scaled_no_opp, y_train)\n",
    "tree.fit(X_train_scaled_no_opp, y_train)\n",
    "\n",
    "# Making predictions with each model\n",
    "log_reg_preds_no_opp = logistic_regression.predict(X_test_scaled_no_opp)\n",
    "tree_preds_no_opp = tree.predict(X_test_scaled_no_opp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine classification report.\n",
    "model_preds_no_opp = {\n",
    "    \"Logistic Regression\": log_reg_preds_no_opp,\n",
    "    \"Boosted Decision Tree\": tree_preds_no_opp,\n",
    "}\n",
    "\n",
    "for model, preds in model_preds_no_opp.items():\n",
    "    print(f\"{model} Results:\\n{classification_report(y_test, preds)}\", sep=\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the confusion matrices.\n",
    "cnf_matrix_log = metrics.confusion_matrix(y_test, log_reg_preds_no_opp)\n",
    "cnf_matrix_tree = metrics.confusion_matrix(y_test, tree_preds_no_opp)\n",
    "\n",
    "class_names=[\"winning\",\"losing\", \"maintaining\"]\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix_log, columns=class_names, index=class_names), cmap=\"YlGnBu\", annot=True, ax=ax[0],fmt='g')\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix_tree, columns=class_names, index=class_names), cmap=\"YlGnBu\", annot=True, ax=ax[1],fmt='g')\n",
    "\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n",
    "\n",
    "ax[0].set_title('Logistic Regression')\n",
    "ax[1].set_title('Boosted Decision Tree')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numbers are very similar in both of the models.\n",
    "\n",
    "If this were something we wanted to pursue, we need to explore:\n",
    "- Definitions of winning, losing, maintain\n",
    "- Fine-tuning the models\n",
    "- Picking models better suited to the problem\n",
    "- Creating a larger sample size\n",
    "- Better feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
